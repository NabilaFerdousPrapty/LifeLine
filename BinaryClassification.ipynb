{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f910d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nabil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Device: cuda\n",
      "‚úÖ Setup & Preprocessing Function Ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# --- Text Preprocessing Imports ---\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download stopwords once\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# --- 1. Setup Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Device: {device}\")\n",
    "\n",
    "# --- 2. Define Preprocessing Function ---\n",
    "def preprocess_text(text):\n",
    "    text = str(text) \n",
    "    # 1. Removal of HTML\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # 2. To Lower Case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. Removal of URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 4. Removal of Twitter Handles (@user)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 5. Removal of Hashtag symbol (keeping text)\n",
    "    text = re.sub(r'#', '', text) \n",
    "    \n",
    "    # 6. Removal of Placeholders\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "    # 7. Removal of Punctuation & Non-letter Characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 8. Removal of Stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # 9. Clean extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"‚úÖ Setup & Preprocessing Function Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e78b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDisasterClassifier(nn.Module):\n",
    "    def __init__(self, model_id, num_classes=2):\n",
    "        super(MultimodalDisasterClassifier, self).__init__()\n",
    "        \n",
    "        print(f\"Loading {model_id} with SafeTensors...\")\n",
    "        self.clip = CLIPModel.from_pretrained(model_id, use_safetensors=True)\n",
    "        \n",
    "        # Increased capacity in the head (Standard 512 -> 256 -> 2)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),   # Added BatchNorm for stability\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),       # Increased Dropout to 0.3\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        text_out = self.clip.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        img_out = self.clip.get_image_features(pixel_values=pixel_values)\n",
    "        \n",
    "        # Normalize\n",
    "        text_out = text_out / text_out.norm(dim=-1, keepdim=True)\n",
    "        img_out = img_out / img_out.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        combined = torch.cat((img_out, text_out), dim=1)\n",
    "        logits = self.classifier(combined.float())\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337b246b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset Class Defined!\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Define Custom Dataset Class ---\n",
    "class CrisisDataset(Dataset):\n",
    "    def __init__(self, df, processor, data_path=\".\"):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['tweet_text']\n",
    "        label = row['label']\n",
    "        img_path = os.path.join(self.data_path, row['image'])\n",
    "        \n",
    "        try:\n",
    "            # Load and convert image\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            # CLIP Processor handles Image Resizing/Norm + Text Tokenization\n",
    "            encoding = self.processor(\n",
    "                text=text, \n",
    "                images=image, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                max_length=77\n",
    "            )\n",
    "            \n",
    "            # Remove batch dimension added by processor\n",
    "            return {\n",
    "                'pixel_values': encoding['pixel_values'].squeeze(0),\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'label': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # If an image fails, return the next one (simple error handling)\n",
    "            return self.__getitem__((idx + 1) % len(self.df))\n",
    "\n",
    "print(\"‚úÖ Dataset Class Defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Applying Text Preprocessing (Steps 1-9)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13608/13608 [00:00<00:00, 14347.72it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2237/2237 [00:00<00:00, 12567.38it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Ready! Train: 13608, Test: 2237\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Load & Clean Data ---\n",
    "# Define paths (Adjust these to match your folder structure)\n",
    "train_path = os.path.join(\"data/CrisisMMD/crisismmd_datasplit_all/crisismmd_datasplit_agreed_label/task_informative_text_img_agreed_lab_train.tsv\")\n",
    "test_path = os.path.join(\"data/CrisisMMD/crisismmd_datasplit_all/crisismmd_datasplit_agreed_label/task_informative_text_img_agreed_lab_test.tsv\")\n",
    "\n",
    "# Load DataFrames\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_path, sep='\\t')\n",
    "\n",
    "# Function to encode labels\n",
    "def encode_label(row):\n",
    "    if row['label_image'] == 'informative': return 1\n",
    "    elif row['label_image'] == 'not_informative': return 0\n",
    "    return None\n",
    "\n",
    "# Apply Label Encoding\n",
    "train_df['label'] = train_df.apply(encode_label, axis=1)\n",
    "test_df['label'] = test_df.apply(encode_label, axis=1)\n",
    "\n",
    "# Drop NaNs\n",
    "train_df = train_df.dropna(subset=['label'])\n",
    "test_df = test_df.dropna(subset=['label'])\n",
    "train_df['label'] = train_df['label'].astype(int)\n",
    "test_df['label'] = test_df['label'].astype(int)\n",
    "\n",
    "# --- APPLY PREPROCESSING HERE ---\n",
    "print(\"‚è≥ Applying Text Preprocessing (Steps 1-9)...\")\n",
    "tqdm.pandas() # Progress bar\n",
    "train_df['tweet_text'] = train_df['tweet_text'].progress_apply(preprocess_text)\n",
    "test_df['tweet_text'] = test_df['tweet_text'].progress_apply(preprocess_text)\n",
    "\n",
    "# Initialize Processor\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create Datasets & Loaders\n",
    "batch_size = 32 # Keep small for GPU memory\n",
    "train_dataset = CrisisDataset(train_df, processor)\n",
    "test_dataset = CrisisDataset(test_df, processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ Data Ready! Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4d6201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading openai/clip-vit-base-patch32 with SafeTensors...\n",
      "trainable params: 5,898,240 || all params: 157,767,299 || trainable%: 3.7386\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- A. RE-INITIALIZE YOUR BASE MODEL (CRITICAL) ---\n",
    "# RESTART YOUR KERNEL BEFORE RUNNING THIS.\n",
    "# Replace \"your-model-id-here\" with your actual model path (e.g., \"openai/clip-vit-base-patch32\")\n",
    "MODEL_ID = \"openai/clip-vit-base-patch32\" # <-- UPDATE THIS ACCORDING TO YOUR THESIS CODE\n",
    "\n",
    "model = MultimodalDisasterClassifier(model_id=MODEL_ID, num_classes=2) \n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# --- B. DEFINE HELPERS ---\n",
    "class MockConfig:\n",
    "    def __init__(self):\n",
    "        self.tie_word_embeddings = False\n",
    "        self.use_return_dict = False\n",
    "    def get(self, key, default=None):\n",
    "        return getattr(self, key, default)\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=4):\n",
    "        self.patience, self.counter, self.best_acc, self.early_stop = patience, 0, 0.0, False\n",
    "    def __call__(self, val_acc):\n",
    "        if val_acc > self.best_acc:\n",
    "            self.best_acc, self.counter = val_acc, 0\n",
    "            return True\n",
    "        self.counter += 1\n",
    "        if self.counter >= self.patience: self.early_stop = True\n",
    "        return False\n",
    "\n",
    "# --- C. APPLY CLEAN LoRA ---\n",
    "model.config = MockConfig()\n",
    "lora_config = LoraConfig(\n",
    "    r=64, \n",
    "    lora_alpha=128, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# \n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- D. TRAINING SETUP ---\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "early_stopper = EarlyStopper(patience=4)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5e4e429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gated Model Ready. Trainable Params: 8565250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [09:55<00:00,  1.40s/it, loss=0.2273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 1 Val Acc: 0.8869 | Avg Loss: 0.5183\n",
      "‚≠ê PERFORMANCE BREAKTHROUGH: New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [12:48<00:00,  1.80s/it, loss=0.4650]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 2 Val Acc: 0.8891 | Avg Loss: 0.3853\n",
      "‚≠ê PERFORMANCE BREAKTHROUGH: New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [14:10<00:00,  2.00s/it, loss=0.2883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 3 Val Acc: 0.8847 | Avg Loss: 0.3428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [13:26<00:00,  1.89s/it, loss=0.2541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 4 Val Acc: 0.8865 | Avg Loss: 0.2920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [14:24<00:00,  2.03s/it, loss=0.3011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 5 Val Acc: 0.8869 | Avg Loss: 0.2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [13:10<00:00,  1.86s/it, loss=0.2348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 6 Val Acc: 0.8833 | Avg Loss: 0.2314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [13:35<00:00,  1.92s/it, loss=0.2261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 7 Val Acc: 0.8815 | Avg Loss: 0.2228\n",
      "üõë Stopping Early. Best Accuracy: 0.8891\n",
      "\n",
      "üèÜ Final Result: 0.8891\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import CLIPModel, get_cosine_schedule_with_warmup\n",
    "from torch.amp import GradScaler, autocast \n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. HELPER CLASSES ---\n",
    "\n",
    "class MockConfig:\n",
    "    \"\"\"Fixes PEFT compatibility issues.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tie_word_embeddings = False\n",
    "        self.use_return_dict = False\n",
    "    def get(self, key, default=None):\n",
    "        return getattr(self, key, default)\n",
    "\n",
    "class EarlyStopper:\n",
    "    \"\"\"Monitors validation accuracy to prevent overfitting.\"\"\"\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience, self.counter, self.best_acc, self.early_stop = patience, 0, 0.0, False\n",
    "    def __call__(self, val_acc):\n",
    "        if val_acc > self.best_acc:\n",
    "            self.best_acc, self.counter = val_acc, 0\n",
    "            return True\n",
    "        self.counter += 1\n",
    "        if self.counter >= self.patience: self.early_stop = True\n",
    "        return False\n",
    "\n",
    "# --- 2. GATED ARCHITECTURE (The Technical Novelty) ---\n",
    "\n",
    "class GatedMultimodalClassifier(nn.Module):\n",
    "    \"\"\"Uses Adaptive Gating to weight modalities dynamically.\"\"\"\n",
    "    def __init__(self, model_id=\"openai/clip-vit-base-patch32\", num_classes=2):\n",
    "        super().__init__()\n",
    "        # use_safetensors=True bypasses 2025 security vulnerability\n",
    "        self.clip = CLIPModel.from_pretrained(model_id, use_safetensors=True)\n",
    "        \n",
    "        # Adaptive Gate: Scales features based on their reliability\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        text_out = self.clip.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        img_out = self.clip.get_image_features(pixel_values=pixel_values)\n",
    "        \n",
    "        # L2 Normalization ensures features are on the same scale\n",
    "        text_f = text_out / text_out.norm(dim=-1, keepdim=True)\n",
    "        img_f = img_out / img_out.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        combined = torch.cat((img_f, text_f), dim=1)\n",
    "        # Gating mechanism suppresses noise in disparate modalities\n",
    "        gated_f = combined * self.gate(combined.float())\n",
    "        \n",
    "        return self.classifier(gated_f)\n",
    "\n",
    "# --- 3. INITIALIZATION & DoRA ---\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GatedMultimodalClassifier(num_classes=NUM_CLASSES).to(device)\n",
    "model.config = MockConfig()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, lora_alpha=64, use_dora=True, # DoRA matches Full FT performance\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"LayerNorm\", \"ln_1\", \"ln_2\", \"ln_final\", \"classifier\", \"gate\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(f\"‚úÖ Gated Model Ready. Trainable Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# --- 4. OPTIMIZER & SCHEDULER ---\n",
    "\n",
    "EPOCHS = 15\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "# Differential Learning Rates stabilize the fine-tuning process\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': [p for n, p in model.named_parameters() if \"classifier\" not in n and \"gate\" not in n], 'lr': 2e-5},\n",
    "    {'params': model.classifier.parameters(), 'lr': 8e-5},\n",
    "    {'params': model.gate.parameters(), 'lr': 1e-4} \n",
    "], weight_decay=0.05)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.15 * total_steps), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1) \n",
    "scaler = GradScaler('cuda')\n",
    "early_stopper = EarlyStopper(patience=5)\n",
    "\n",
    "# --- 5. TRAINING LOOP ---\n",
    "\n",
    "\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch in loop:\n",
    "        ids, mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "        pixels, labels = batch['pixel_values'].to(device), batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'): \n",
    "            outputs = model(ids, mask, pixels)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad(), autocast('cuda'):\n",
    "        for batch in test_loader:\n",
    "            v_ids, v_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "            v_pixels, v_labels = batch['pixel_values'].to(device), batch['label'].to(device)\n",
    "            out = model(v_ids, v_mask, v_pixels)\n",
    "            correct += (torch.argmax(out, 1) == v_labels).sum().item()\n",
    "            total += v_labels.size(0)\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    print(f\"üìâ Epoch {epoch+1} Val Acc: {val_acc:.4f} | Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    if early_stopper(val_acc):\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_informativeness_gated.pth\")\n",
    "        print(\"‚≠ê PERFORMANCE BREAKTHROUGH: New Best Model Saved!\")\n",
    "\n",
    "    if early_stopper.early_stop:\n",
    "        print(f\"üõë Stopping Early. Best Accuracy: {best_acc:.4f}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüèÜ Final Result: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296d67a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting the Symmetric Fusion Elite Run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 406/426 [21:00<01:06,  3.33s/it]d:\\Anaconda\\envs\\tweet_project\\lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [22:05<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 1 Val Acc: 0.8891 | Loss: 0.0146\n",
      "‚≠ê BREAKTHROUGH: Best Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [20:13<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 2 Val Acc: 0.8918 | Loss: 0.0096\n",
      "‚≠ê BREAKTHROUGH: Best Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [24:44<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 3 Val Acc: 0.8847 | Loss: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [20:13<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 4 Val Acc: 0.8829 | Loss: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [18:52<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 5 Val Acc: 0.8824 | Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [16:36<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Epoch 6 Val Acc: 0.8820 | Loss: 0.0030\n",
      "üèÜ Final Result: 0.8918\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import CLIPModel, get_cosine_schedule_with_warmup\n",
    "from torch.amp import GradScaler, autocast \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. HELPER CLASSES ---\n",
    "\n",
    "class MockConfig:\n",
    "    def __init__(self):\n",
    "        self.tie_word_embeddings = False\n",
    "        self.use_return_dict = False\n",
    "    def get(self, key, default=None):\n",
    "        return getattr(self, key, default)\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience, self.counter, self.best_acc, self.early_stop = patience, 0, 0.0, False\n",
    "    def __call__(self, val_acc):\n",
    "        if val_acc > self.best_acc:\n",
    "            self.best_acc, self.counter = val_acc, 0\n",
    "            return True\n",
    "        self.counter += 1\n",
    "        if self.counter >= self.patience: self.early_stop = True\n",
    "        return False\n",
    "\n",
    "class StableFocalLoss(nn.Module):\n",
    "    \"\"\"Focuses on the final 10% of 'hard' ambiguous disaster tweets.\"\"\"\n",
    "    def __init__(self, gamma=3.0, alpha=0.25):\n",
    "        super().__init__()\n",
    "        self.gamma, self.alpha = gamma, alpha\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        return (self.alpha * (1 - pt)**self.gamma * ce_loss).mean()\n",
    "\n",
    "# --- 2. ELITE ARCHITECTURE: SYMMETRIC DUAL-PATH ATTENTION ---\n",
    "\n",
    "class SymmetricBCAFusion(nn.Module):\n",
    "    \"\"\"Implements mutual grounding between text and visual features.\"\"\"\n",
    "    def __init__(self, embed_dim=512, heads=8):\n",
    "        super().__init__()\n",
    "        self.text_to_img = nn.MultiheadAttention(embed_dim, heads, batch_first=True)\n",
    "        self.img_to_text = nn.MultiheadAttention(embed_dim, heads, batch_first=True)\n",
    "        self.gate = nn.Sequential(nn.Linear(embed_dim * 2, embed_dim * 2), nn.Sigmoid())\n",
    "        self.norm = nn.LayerNorm(embed_dim * 2)\n",
    "\n",
    "    def forward(self, text_f, img_f):\n",
    "        # Path A: Text finds evidence in Image\n",
    "        aligned_t, _ = self.text_to_img(text_f.unsqueeze(1), img_f.unsqueeze(1), img_f.unsqueeze(1))\n",
    "        # Path B: Image finds context in Text\n",
    "        aligned_i, _ = self.img_to_text(img_f.unsqueeze(1), text_f.unsqueeze(1), text_f.unsqueeze(1))\n",
    "        \n",
    "        combined = torch.cat((aligned_t.squeeze(1), aligned_i.squeeze(1)), dim=1)\n",
    "        # Residual Gating: Stabilizes learning and prevents feature suppression\n",
    "        return self.norm(combined * self.gate(combined) + torch.cat((text_f, img_f), dim=1))\n",
    "\n",
    "class EliteDualFusionClassifier(nn.Module):\n",
    "    def __init__(self, model_id=\"openai/clip-vit-base-patch32\", num_classes=2):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(model_id, use_safetensors=True)\n",
    "        self.fusion = SymmetricBCAFusion()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.GELU(),\n",
    "            nn.Dropout(0.5), nn.Linear(512, 128), nn.GELU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        t_f = self.clip.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        i_f = self.clip.get_image_features(pixel_values=pixel_values)\n",
    "        t_f, i_f = t_f / t_f.norm(dim=-1, keepdim=True), i_f / i_f.norm(dim=-1, keepdim=True)\n",
    "        return self.classifier(self.fusion(t_f, i_f))\n",
    "\n",
    "# --- 3. TRAINING INITIALIZATION ---\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EliteDualFusionClassifier().to(device)\n",
    "model.config = MockConfig()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64, lora_alpha=128, use_dora=True, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.1, modules_to_save=[\"LayerNorm\", \"classifier\", \"fusion\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(x in n for x in [\"classifier\", \"fusion\"])], 'lr': 5e-6},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-4}, \n",
    "    {'params': model.fusion.parameters(), 'lr': 1e-4}\n",
    "], weight_decay=0.15) # High decay forces the model to generalize\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * len(train_loader)*15), num_training_steps=len(train_loader)*15\n",
    ")\n",
    "criterion = StableFocalLoss(gamma=3.0) \n",
    "scaler = GradScaler('cuda')\n",
    "early_stopper = EarlyStopper(patience=4)\n",
    "\n",
    "# --- 4. THE PUSH TO 91% ---\n",
    "\n",
    "\n",
    "print(\"üöÄ Starting the Symmetric Fusion Elite Run...\")\n",
    "best_acc = 0.0\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        ids, mask, pixels, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['pixel_values'].to(device), batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            loss = criterion(model(ids, mask, pixels), labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad(), autocast('cuda'):\n",
    "        for batch in test_loader:\n",
    "            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['pixel_values'].to(device))\n",
    "            correct += (torch.argmax(out, 1) == batch['label'].to(device)).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    print(f\"üìâ Epoch {epoch+1} Val Acc: {val_acc:.4f} | Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    if early_stopper(val_acc):\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_symmetric_fusion.pth\")\n",
    "        print(\"‚≠ê BREAKTHROUGH: Best Model Saved.\")\n",
    "\n",
    "    if early_stopper.early_stop: break\n",
    "\n",
    "print(f\"üèÜ Final Result: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d3c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nabil\\AppData\\Local\\Temp\\ipykernel_14276\\3891510328.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_symmetric_fusion.pth\"))\n",
      "d:\\Anaconda\\envs\\tweet_project\\lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import torch\n",
    "\n",
    "# --- 1. EVALUATION DATA COLLECTION ---\n",
    "def get_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad(), autocast('cuda'):\n",
    "        for batch in loader:\n",
    "            ids, mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "            pixels, labels = batch['pixel_values'].to(device), batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(ids, mask, pixels)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy()[:, 1]) # Probability for 'Informative' class\n",
    "            \n",
    "    return np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# Load the best weights before evaluation\n",
    "model.load_state_dict(torch.load(\"best_symmetric_fusion.pth\"))\n",
    "y_pred, y_true, y_probs = get_predictions(model, test_loader, device)\n",
    "\n",
    "# --- 2. PLOT CONFUSION MATRIX ---\n",
    "# Essential for identifying which disaster tweets the model still finds 'ambiguous'\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-Informative', 'Informative'],\n",
    "            yticklabels=['Non-Informative', 'Informative'])\n",
    "plt.title('Confusion Matrix: Symmetric Dual-Path Attention')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# --- 3. PLOT ROC CURVE ---\n",
    "# Proves the model's 'Situational Awareness' across varying thresholds\n",
    "fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# --- 4. CLASSIFICATION REPORT ---\n",
    "print(\"\\nüìù Detailed Performance Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Non-Informative', 'Informative']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
